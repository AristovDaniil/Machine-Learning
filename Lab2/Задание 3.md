# Почему задача быстро сходится от менее чем -1000 до почти оптимального значения?
В начале обучения политика равномерно случайна: каждая возможная команда (действие) имеет равные шансы быть выбрана. Это приводит к неэффективным стратегиям, которые дают низкие вознаграждения (значения около -1000).
Алгоритм начинает отбирать "элитные" сессии (на основе выбранного процентиля, например, 50%), в которых награды выше среднего. Это стимулирует улучшение политики, поскольку она обучается на данных из успешных сессий.
Также на начальных этапах игры часто происходят "легкие" улучшения: из-за равномерной случайной политики многие действия близки к оптимальным, особенно в простых начальных состояниях (например, неудачные действия отбрасываются).
Большой процентиль (например, 50%) помогает ускорить обучение, так как для обновления политики используется большее количество успешных состояний.
# Почему значения наград затем снова снижаются до диапазона -50/-100?
Если алгоритм начинает слишком сильно полагаться на сессии с высшими наградами, это может привести к "зажатию" политики: она становится менее вероятностной и более детерминированной, выбирая узкий набор действий.
При этом менее популярные, но потенциально полезные действия могут быть игнорированы.
В Taxi-v3 есть множество различных начальных состояний и ситуаций. Оптимизация под один набор сценариев может ухудшить производительность в других.
При этом штрафы за некорректные действия (например, посадку/высадку пассажира в неверной локации) в этой игре довольно высоки, что приводит к снижению средней награды.
Неправильно выбранный процентиль (например, слишком высокий) может привести к уменьшению разнообразия политики, так как отбираются только самые успешные сессии.
Если обучение недостаточно долгое или скорость обучения (learning rate) велика, алгоритм может "дрейфовать", не успевая стабилизироваться на оптимальной политике.
На начальном этапе алгоритм быстро улучшает случайную политику. Однако в дальнейшем сложные улучшения требуют более точного и длительного обучения, что может быть замедлено, особенно при снижении разнообразия данных.
# Итог
Если подытожить, то можно сказать, что задача Taxi-v3 демонстрирует быстрое схождение на начальных этапах, поскольку улучшение случайной политики дает значительное повышение награды.
Однако позже награда может снижаться из-за уменьшения разнообразия политики, сложности среды, и возможного избыточного отбора успешных сессий. Этот эффект можно смягчить, используя более плавное уменьшение процентиля, уменьшение скорости обучения, или введение регуляризации в обновление политики.
